name: Deploy to AWS EKS

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  AWS_REGION: us-east-1
  EKS_CLUSTER_NAME: whatsapp-analytics-production
  ECR_REPOSITORY: datacompass-api
  IMAGE_TAG: ${{ github.sha }}
  NAMESPACE: datacompass

jobs:
  validate-prerequisites:
    name: Validate Prerequisites
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Validate required secrets
      run: |
        if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ]; then
          echo "❌ AWS_ACCESS_KEY_ID secret is not set"
          exit 1
        fi
        if [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then
          echo "❌ AWS_SECRET_ACCESS_KEY secret is not set"
          exit 1
        fi
        if [ -z "${{ secrets.AWS_ACCOUNT_ID }}" ]; then
          echo "❌ AWS_ACCOUNT_ID secret is not set"
          exit 1
        fi
        echo "✅ All required secrets are configured"

  test:
    name: Run Tests
    runs-on: ubuntu-latest
    needs: validate-prerequisites
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        
    - name: Install dependencies
      run: npm install
      
    - name: Type check
      run: npm run type-check
      
    - name: Run linting
      run: npm run lint
      
    - name: Run tests
      run: npm test
      
    - name: Build application
      run: npm run build

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: validate-prerequisites
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'table'
        
    - name: Run Trivy for high/critical vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'table'
        severity: 'HIGH,CRITICAL'
        exit-code: '0'

  build-and-push:
    name: Build and Push to ECR
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.ref == 'refs/heads/main'
    outputs:
      image_uri: ${{ steps.build-image.outputs.image_uri }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
      
    - name: Build and tag Docker image
      id: build-image
      run: |
        # Verifica se AWS_ACCOUNT_ID está definido
        if [ -z "${{ secrets.AWS_ACCOUNT_ID }}" ]; then
          echo "❌ Error: AWS_ACCOUNT_ID secret is not configured"
          exit 1
        fi
        # Garante que IMAGE_TAG sempre tem valor
        IMAGE_TAG="${{ env.IMAGE_TAG }}"
        if [ -z "$IMAGE_TAG" ]; then
          IMAGE_TAG="latest"
        fi
        IMAGE_URI="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG"
        LATEST_URI="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPOSITORY }}:latest"
        echo "✅ Building image: $IMAGE_URI"
        docker build -t $IMAGE_URI .
        docker tag $IMAGE_URI $LATEST_URI
        echo "image-uri=$IMAGE_URI" >> $GITHUB_OUTPUT
        echo "✅ Image built and tagged successfully"
        
    - name: Debug image-uri para scan
      run: |
        echo "IMAGE_URI para scan: ${{ steps.build-image.outputs.image-uri }}"

    - name: Scan Docker image
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: '${{ steps.build-image.outputs.image-uri }}'
        format: 'table'
        severity: 'HIGH,CRITICAL'
        exit-code: '0'
        
    - name: Push image to ECR
      run: |
        docker push ${{ steps.build-image.outputs.image_uri }}
        docker push ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPOSITORY }}:latest

  deploy-to-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build-and-push
    if: github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Debug outputs recebidos
      run: |
        echo "Outputs recebidos do build-and-push: ${{ toJson(needs.build-and-push.outputs) }}"
        echo "IMAGE_URI recebido: ${{ needs.build-and-push.outputs.image_uri }}"
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
        
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
    - name: Create namespace if not exists
      run: |
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        
    - name: Deploy to staging
      run: |
        # Verify image URI is available
        IMAGE_URI="${{ needs.build-and-push.outputs.image_uri }}"
        if [ -z "$IMAGE_URI" ]; then
          echo "❌ Error: Image URI is empty"
          echo "Available outputs: ${{ toJson(needs.build-and-push.outputs) }}"
          exit 1
        fi
        
        echo "✅ Using image: $IMAGE_URI"
        
        # Create temporary deployment file with correct image
        cp k8s/base/deployment.yaml deployment-temp.yaml
        sed -i "s|IMAGE_TAG_PLACEHOLDER|$IMAGE_URI|g" deployment-temp.yaml
        
        # Verify the replacement worked
        if grep -q "IMAGE_TAG_PLACEHOLDER" deployment-temp.yaml; then
          echo "❌ Error: IMAGE_TAG_PLACEHOLDER was not replaced"
          cat deployment-temp.yaml
          exit 1
        fi
        
        echo "✅ Image replacement successful"
        
        # Apply Kubernetes manifests
        kubectl apply -f k8s/base/namespace.yaml || echo "Namespace already exists"
        kubectl apply -f k8s/base/configmap.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/secret.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f deployment-temp.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/service.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/hpa.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/ingress.yaml -n ${{ env.NAMESPACE }}
        
        # Clean up temporary file
        rm deployment-temp.yaml
        
    - name: Wait for deployment
      run: |
        kubectl rollout status deployment/whatsapp-analytics-api -n ${{ env.NAMESPACE }} --timeout=300s
        
    - name: Run health checks
      run: |
        echo "🧪 Running health checks..."
        
        # Wait for pods to be ready
        kubectl wait --for=condition=ready pod -l app=whatsapp-analytics-api -n ${{ env.NAMESPACE }} --timeout=300s
        
        # Get service endpoint
        SERVICE_NAME="whatsapp-analytics-api-service"
        
        # Use port-forward for health check
        kubectl port-forward service/$SERVICE_NAME 8080:80 -n ${{ env.NAMESPACE }} &
        PORT_FORWARD_PID=$!
        
        # Wait for port-forward to be ready
        sleep 10
        
        # Run health check
        for i in {1..5}; do
          if curl -f http://localhost:8080/health; then
            echo "✅ Health check passed"
            break
          else
            echo "⚠️ Health check attempt $i failed, retrying..."
            sleep 10
          fi
          
          if [ $i -eq 5 ]; then
            echo "❌ Health check failed after 5 attempts"
            kill $PORT_FORWARD_PID || true
            exit 1
          fi
        done
        
        # Clean up port-forward
        kill $PORT_FORWARD_PID || true
        
        echo "✅ Staging deployment completed successfully!"

  deploy-to-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [build-and-push, deploy-to-staging]
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest'
        
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.EKS_CLUSTER_NAME }}
        
    - name: Create namespace if not exists
      run: |
        kubectl create namespace ${{ env.NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -
        
    - name: Backup current deployment
      run: |
        kubectl get deployment whatsapp-analytics-api -n ${{ env.NAMESPACE }} -o yaml > backup-deployment.yaml || echo "No existing deployment to backup"
        
    - name: Deploy to production
      run: |
        # Create temporary deployment file with correct image
        cp k8s/base/deployment.yaml deployment-temp.yaml
        sed -i "s|IMAGE_TAG_PLACEHOLDER|${{ needs.build-and-push.outputs.image_uri }}|g" deployment-temp.yaml
        
        # Apply Kubernetes manifests
        kubectl apply -f k8s/base/namespace.yaml || echo "Namespace already exists"
        kubectl apply -f k8s/base/configmap.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/secret.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f deployment-temp.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/service.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/hpa.yaml -n ${{ env.NAMESPACE }}
        kubectl apply -f k8s/base/ingress.yaml -n ${{ env.NAMESPACE }}
        
        # Clean up temporary file
        rm deployment-temp.yaml
        
    - name: Wait for deployment
      run: |
        if ! kubectl rollout status deployment/whatsapp-analytics-api -n ${{ env.NAMESPACE }} --timeout=600s; then
          echo "❌ Deployment failed, initiating rollback"
          kubectl rollout undo deployment/whatsapp-analytics-api -n ${{ env.NAMESPACE }}
          exit 1
        fi
        
    - name: Run production health checks
      run: |
        echo "🚀 Running production health checks..."
        
        # Wait for pods to be ready
        kubectl wait --for=condition=ready pod -l app=whatsapp-analytics-api -n ${{ env.NAMESPACE }} --timeout=300s
        
        # Use port-forward for health check
        kubectl port-forward service/whatsapp-analytics-api-service 8080:80 -n ${{ env.NAMESPACE }} &
        PORT_FORWARD_PID=$!
        
        # Wait for port-forward to be ready
        sleep 15
        
        # Run comprehensive health checks
        HEALTH_CHECK_PASSED=false
        for i in {1..10}; do
          if curl -f http://localhost:8080/health && curl -f http://localhost:8080/api/whatsapp/status; then
            echo "✅ All health checks passed"
            HEALTH_CHECK_PASSED=true
            break
          else
            echo "⚠️ Health check attempt $i failed, retrying..."
            sleep 15
          fi
        done
        
        # Clean up port-forward
        kill $PORT_FORWARD_PID || true
        
        if [ "$HEALTH_CHECK_PASSED" = false ]; then
          echo "❌ Health checks failed, initiating rollback"
          kubectl rollout undo deployment/whatsapp-analytics-api -n ${{ env.NAMESPACE }}
          exit 1
        fi
        
        echo "🚀 Production deployment completed successfully!"
        
    - name: Notify deployment success
      if: success()
      run: |
        echo "🎉 DataCompass 1.0 deployed successfully to production!"
        echo "📊 Deployment Details:" >> $GITHUB_STEP_SUMMARY
        echo "- **Image**: ${{ needs.build-and-push.outputs.image_uri }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Namespace**: ${{ env.NAMESPACE }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Cluster**: ${{ env.EKS_CLUSTER_NAME }}" >> $GITHUB_STEP_SUMMARY
        
    - name: Notify deployment failure
      if: failure()
      run: |
        echo "❌ Production deployment failed!"
        echo "🔄 Automatic rollback has been initiated"

  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [deploy-to-production]
    if: always()
    
    steps:
    - name: Notify Slack
      if: always() && env.SLACK_WEBHOOK_URL != ''
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        fields: repo,message,commit,author,action,eventName,ref,workflow
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        
    - name: Summary
      run: |
        echo "## 🚀 Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- **Repository**: ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Status**: ${{ needs.deploy-to-production.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Image Tag**: ${{ env.IMAGE_TAG }}" >> $GITHUB_STEP_SUMMARY

